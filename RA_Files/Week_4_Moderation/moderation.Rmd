---
title: "Multiple Regression with Interactions"
author: 
  - "Emorie D Beck"
date: "`r Sys.setlocale('LC_TIME', 'C'); format(Sys.time(), '%d\\\\. %B %Y')`"
output:
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
    toc: true
    toc_float: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Workspace 

## Packages
```{r}
library(psych)
library(broom)
library(plyr)
library(tidyverse)
```

## Data  
This week, our data are going to come from the German Socioeconomic Panel Study (GSOEP). The GSOEP is a longitudinal study of adults in German housesholds. The study has a broad range of variables, but for our purposes we're just going to use personality ratings, age, self-rated health, and gender from a single year (2005). In future weeks, we'll work to get additional variables across waves, which will be more involved, but this will get our feet wet with these data. 

Like when we used the PAIRS data, we need a codebook. To create it, go to https://data.soep.de/soep-core# and use the search feature to find the variables you need or https://data.soep.de/soep-long/topics/ where you can scroll through topics (this may be easier for finding the personality variables). I've included a base codebook that should show you what you need to gather for each variable.  
```{r}
wd <- "https://github.com/emoriebeck/R-tutorials/raw/master/RA_Files/Week_4_Moderation"
dat.2005 <- sprintf("%s/vp.sav", wd) %>% haven::read_spss()
```

These data are organized differently than others we've worked with in the past. The data are actually split into multiple .sav files for each year (the "v" above means 2005). We're just going to work with 1 that will include the variables we need. 

Once you've got the codebook, we should be ready to go. 

To start, let's trim out data frame. 
```{r}
# create a vector of the old item names we need. 
# old_names <- codebook$old_names
# new_names <- codebook$new_names

# use your preferred method to get rid of columns we don't need 
# and rename them using our character vector of new names  

```

## Descriptives
```{r}
# run the descriptives and check variable ranges

```

Now that we have all the variables, we need to give them useful names. To do this, we need to mix some info from different objects. This is going to be hard. I'm going to tell you what pieces you need, but I'm going to leave it up to you to try out different ways to bring them all together into a data frame. Think about how we created the codebook last week. You're aiming to create a data frame that has those same elements.  

It should end up looking something like: 

old\_item   |   new\_item     | item\_text    | keys    
----------  | --------------  | ------------- | ---------------

```{r}
# get the original item names from the ItemLists object 
# hint: filter out the rows in the IPIP100 column that are null

# give them new names 
# hint: use paste to create new item names like "IPIP100.A_1"

# get the keys; make sure you match them with the correct items and add them to the data frame
```

## Check Missings 
How are missings coded in this data set? Do we need to make any changes to how they are coded?  
```{r}
# You should have noted some variables that needed "scrubbed" (changed to missing)
# change those to NA using your preferred method

```

## Recode Variables  
```{r}
# You should have your keys. Reverse code the items that need reverse coded. 

```

## Create composites
For these data, we have lots of items, so we don't just want to create composites for the Big 5, we also want to create composites for the facets of each of the Big 5. Use the methods we learned before to do so.  
```{r}

```

# Zero-Order Correlations
Before we run a regression, we should always look at the zero-order correlations among the predictors and outcomes. For these data, we want to look at the relationship between age and personality, and gender, so correlate the age column with the composites for each of the Big 5 and their facets.  
```{r}
# run the correlations

```

# Moderation  
Last week, we looked at cross-sectional age differences in personality, controlling for background characteristics. This week, we're going to look at how those characteristics may *moderate* the relationship between age and personality. One way to think about this is that it means that personality varies as a function of both age and whatever moderator variable we are choosing. 

This shouldn't be a foreign concept, as it's the same thing you learned when you learned ANOVA in your intro stats course. In fact, ANOVA *is* regression. And this week, I'll show you why.  

But let's start with an equation. When we were doing simple regression before, our regression equation was simply:  
$$Y_{ij} = b_0 + b_1X_1 + \epsilon_{ij}$$  
where Y is the outcome variable (personality), X is the predictor variable (age) and $\epsilon_{ij}$ are the residuals of the regression line.  $b_0$ is the intercept (mean at X = 0) and $b_1$ is the scaled relationship between X and Y ($b_1 = r_{XY}\frac{s_y}{s_x}$).  

Now, we can directly incorporate the error into the measurement by subbing $Y_{ij}$ for $\hat{Y}_{ij}$, which signals we are only interested in the predicted value. The error / uncertainty will still be reflected in the standard errors of the coefficients.  
$$\hat{Y}_{ij} = b_0 + b_1X_1$$  

When we switched to the multiple regression case, this equation became: 

$$\hat{Y}_{ij} = b_0 + b_1X_1 + b_2X_2 + ... + b_pX_p$$  
where p is the number of predictors in the model. 

Now last time, I mentioned that we wouldn't get into the prediction question with multiple regression because it's more complicated. This is because we have to control for the covariates when making predictions. How we do that makes sense when we consider an alternative interpretation of any of the slope coefficients in multiple regression. "For someone with average levels of the covariates, a 1 unit increase in $X_i$ is associated with a $b_1$ increase in Y." Hopefully that makes it clear that what we have to do to find the predicted value $\hat{Y}$ at a given level on a predictor X, is to set all of the predictors/covariates to the average values. 

```{r}
fitE2 <- lm(BFI.E ~ age + BFI.A, data = dat.2005)
```

Now, I'm going to introduce a useful new function to help us create the data frame we need to get the predicted values: `crossing()` from the `dplyr` package. Essentially, crossing is a function that "crosses" all the levels / values of the variables that you give it. It results in a data frame with rows that have all possible combinations of the columns. This is really great because otherwise you are stuck using `rep()` 8000 times.  

```{r}
pred.dat.E1 <- 
  crossing(
    age = seq(10, 80, 1),
    BMI = mean(dat.2005$BFI.A, na.rm = T)
  ) %>%
  mutate(pred = predict(fitE2, newdata = .))

pred.dat.E1 %>%
  ggplot(aes(x = age, value = pred)) +
  geom_line() +
  labs(x = "Age", y = "Predicted Extraversion Rating") +
  theme_classic()
```

Moderation can be partially thought of as an extension of multiple regression, where we aren't just controlling for a variable as a covariate, we're saying that the relationship between the predictor X and the outcome Y varies at different levels of the covariate. Consider, for example, depression and positive events' relationship to global happiness ratings. If you think about it, you wouldn't expect positive events to affect the happiness of people with different levels of depression the same. There's a reason that one type of depression is called anhedonia -- positive events stop arousing positive emotions.  

## Categorical Moderators  
We're going to start with the simplest form of moderators: a two-level nominal variable. Gender is a good variable to use for this. For our personality example, we might ask if gender moderates age differences in personality. Stated differently, this tests whether the age differences in personality across the life span are different for men and women.  

The equation for this is as follows: 

$$\hat{Y}_{ij} = b_0 + b_1*age + b_2*BFI.A + b_3*(age*BFI.A)$$
Our new estimate ($b_3$) is our interaction. For the $b_1$ and $b_2$ coefficients, we get predicted estimates by feeding in literal values of age or Agreeableness. For an interaction, the values are going to be products between the two variables we are interacting. So in this case the values that would get multiplied by $b_3$ are each person's age multiplied by their Agreeableness score. 

So the model frame would something like this: 
```{r}
dat.2005 %>% select(age, gender) %>% mutate(`age:gender` = age * gender)
```

The good news is that we don't generally have to do this R. If we are using the `lm()` function in R, then it will do this for us in the background. We just have to communicate to `lm()` that we want to add an interaction. 

In this case, we can tell R we want an interaction by specifying `lm(BFI.E ~ age + gender + age:gender, data = dat.2005)`. An equivalent form is `lm(BFI.E ~ age*gender, data = dat.2005)`, but be careful using that notation becuase it assumes you want main effects for all variables associated with the "*" and that may not always be the case.  

Anyway, we're now set to run the model: 

```{r}
# run the model below and save it to an object. 
```

Does gender moderate the relationship between age and personality?  

How would you interpret the age (main effect) coefficient?  

How would you interpret the gender (main effect) coefficient?  

How would you interpret the interaction coefficient?  

### Plotting  
Plotting for moderation is slightly more complicated. When you learned ANOVA, you may remember learning about post-hoc tests. Those are certainly things we can still run within regression, but when we have continuous predictors (e.g. age) that doesn't make quite as much sense. Instead, we typically run what we call "simple slopes analysis." The basic idea is that to unpack an interaction, which implies that the relationship between one predictor and the outcome varies depending on another predictor, we can hold one of the predictors "constant" at different levels and look at the relationship between the other predictor and the outcome at those levels.  

We started with one nominal variable here because I think that will make this clearer. With an interaction with a nominal variable, we are basically fitting separate regression for men and women to look at the relationship between personality and age. This means that we can also plot these separate lines. With nominal vairbales, it's easy to hold a predictor "constant" -- you just set the predictor equal to the values associated with a nominal variable. Remember that with a nominal variable, we create dummy codes. 

For gender, this would be one dummy coded variable, which means that X will be 0 when the participant is a male and 1 otherwise. 

We can use this an our equation from earlier to estimate the separate equations. 

Overall: $\hat{Y}_{ij} = b_0 + b_1*age + b_2*BFI.A + b_3*(age*BFI.A)$
Males (gender = 0): $\hat{Y}_{ij} = b_0 + b_1*age + b_2*0 + b_3*(age*0) = b_0 + b_1*age$  
Females (gender = 1): $\hat{Y}_{ij} = b_0 + b_1*age + b_2*1 + b_3*(age*1) = (b_0 + b_2) + (b_1 + b_3)*age$  
For females, I've grouped the coefficients because it helps us to interpret the coefficients. 

The $b_2$ coefficient is the mean difference in personality at age = 0. Thus, the estimate for females is the estimate at age = 0 + the difference. The $b_3$ coefficient is the difference in age differences across the lifespan for women relative to men. Thus, the estimate for females is the slope for males ($b_1$) plus the difference in age differences ($b_3$). Notably, both are multiplied by age because they are "slope" estimates.  

The good news is that this is relatively easy to do in R:
```{r}
# remember the crossing function I introduced earlier? Well, it's baaaackk

(out <- crossing(age = seq(10,80,1),
         gender = c("Male", "Female")))

out %>% 
  mutate(pred = predict(fit.E, newdata = .)) %>%
  ggplot(aes(x = age, y = pred, color = gender)) +
  geom_line() +
  theme_classic()
```

## Continuous Moderators  
Continous moderators can be seen as a slightly more complicated version of the nominal moderator case. In this case, we don't have the nominal variable with as a natural variable on which to break down the model into simple slopes. Instead we have to use an alternative method.

But first, we'll start with an illustrative example. Consider the relationship between age, personality, and health. If you think about this for a second, you likely quickly realize that these variables are confounded because we've seen that personality and age are, of course, very related. As a result, we might expect personality to be differentially related to health at different ages. In other words, health depends on both personality and age. From a theoretical perspective, think about Neuroticism. When you're young, Neuroticism may be a good thing. Neurotic young people are probably the ones slathering themselves with sun screen, taking their vitamins, and eating well, particularly if they are also high in Conscientiousness. 

Let's see what this model looks like. In this case, health will be our outcome, and age and personality will be our outcomes. 

$$\hat{Y}_{ij} = b_0 + b_1*age + b_2*personality + b_3*(age*personality)$$

The same principles as before apply -- the $b_3$ coefficient is a product variable that we would create as follows:  
```{r}
dat.2005 %>% select(age, BFI.E) %>% mutate(`age:E` = age * BFI.E)
```

We're now set to run the model: 

```{r}
# run the model below and save it to an object. 

```

Does age moderate the relationship between personality and health?  

How would you interpret the age (main effect) coefficient?  

How would you interpret the personality (main effect) coefficient?  

How would you interpret the interaction coefficient?  

Now, we're ready to start thinking about how we would understand the breakdown of the interaction -- that is, the simple effects. Unlike the case of the nominal predictor, we don't have two levels of one of our predictors to use to break down the relationships (We can't look at the equations for males versus women). Instead, what we typically do is look at the relationship between one predictor and outcome at different levels of the other predictor. What levels do we choose? typically we choose -1 SD, the mean, and +1 SD. Remember from learning about distributions that this means we should be capturing more than 60% of responses, so it's a reasonable approximation. 

For simplicity, let's say our variables are standardized, so that our three values of X are -1, 0, and 1.  

personality = -1: $\hat{Y}_{ij} = b_0 + b_1*age + b_2*-1 + b_3(age*-1) = (b_0 - b_2) + (b_1 - b_3)*age$  
The intercept at -1 would be $(b_0 - b_2)$, while the slope would be $(b_1 - b_3)$  
personality = 0: $\hat{Y}_{ij} = b_0 + b_1*age = (b_0 - b_3) + (b_1 - b_3)*age$  
The intercept at 0 would be $b_0$, while the slope would be $b_1$  
personality = 1: $\hat{Y}_{ij} = b_0 + b_1*age + b_2*1 + b_3(age*1) = (b_0 + b_2) + (b_1 + b_3)*age$  
The intercept at 1 would be $(b_0 + b_2)$, while the slope would be $(b_1 + b_3)$  



### Plotting
Hopefully, this will make more sense once we've plotted the interaction. 

To do this, we'll use a very similar procedure as we used previously, but this time we need to calculate the mean and SD of the variable we want to treat as the moderator 
```{r}
m.BFI.E <- mean(dat.2005$BFI.E, na.rm = T)
sd.BFI.E <-  sd(dat.2005$BFI.E, na.rm = T)

crossing(
  age = seq(10,80,1),
  BFI.E = c(m.BFI.E-sd.BFI.E, m.BFI.E)
) %>%
  mutate(pred = predict(fit.E2))
```


# Comparison to ANOVA  


